{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T07:10:55.315988Z",
     "start_time": "2025-07-21T07:10:55.217336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from llama_cpp import Llama\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Files map (edit paths as needed)\n",
    "METRIC_FILES = {\n",
    "    \"Rmsd\": \"RMSD_data.csv\",\n",
    "    \"Skil\": \"Skil_data.csv\",\n",
    "    \"Bias\": \"bias_data.csv\"\n",
    "}"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mjson\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpd\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mllama_cpp\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Llama\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mIPython\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mdisplay\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m display, Markdown\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/stofschatbot/lib/python3.11/site-packages/pandas/__init__.py:139\u001B[39m\n\u001B[32m    121\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mcore\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mreshape\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mapi\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m    122\u001B[39m     concat,\n\u001B[32m    123\u001B[39m     lreshape,\n\u001B[32m   (...)\u001B[39m\u001B[32m    135\u001B[39m     qcut,\n\u001B[32m    136\u001B[39m )\n\u001B[32m    138\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m api, arrays, errors, io, plotting, tseries\n\u001B[32m--> \u001B[39m\u001B[32m139\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m testing\n\u001B[32m    140\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mutil\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_print_versions\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m show_versions\n\u001B[32m    142\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mio\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mapi\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m    143\u001B[39m     \u001B[38;5;66;03m# excel\u001B[39;00m\n\u001B[32m    144\u001B[39m     ExcelFile,\n\u001B[32m   (...)\u001B[39m\u001B[32m    172\u001B[39m     read_spss,\n\u001B[32m    173\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/stofschatbot/lib/python3.11/site-packages/pandas/testing.py:6\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m      2\u001B[39m \u001B[33;03mPublic testing utility functions.\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpandas\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01m_testing\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[32m      7\u001B[39m     assert_extension_array_equal,\n\u001B[32m      8\u001B[39m     assert_frame_equal,\n\u001B[32m      9\u001B[39m     assert_index_equal,\n\u001B[32m     10\u001B[39m     assert_series_equal,\n\u001B[32m     11\u001B[39m )\n\u001B[32m     13\u001B[39m __all__ = [\n\u001B[32m     14\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33massert_extension_array_equal\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     15\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33massert_frame_equal\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     16\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33massert_series_equal\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     17\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33massert_index_equal\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m     18\u001B[39m ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/stofschatbot/lib/python3.11/site-packages/pandas/_testing/__init__.py:405\u001B[39m\n\u001B[32m    400\u001B[39m     \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mpytest\u001B[39;00m\n\u001B[32m    402\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m pytest.raises(expected_exception, match=\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m--> \u001B[39m\u001B[32m405\u001B[39m cython_table = pd.core.common._cython_table.items()\n\u001B[32m    408\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget_cython_table_params\u001B[39m(ndframe, func_names_and_expected):\n\u001B[32m    409\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    410\u001B[39m \u001B[33;03m    Combine frame, functions from com._cython_table\u001B[39;00m\n\u001B[32m    411\u001B[39m \u001B[33;03m    keys and expected result.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    423\u001B[39m \u001B[33;03m        List of three items (DataFrame, function, expected result)\u001B[39;00m\n\u001B[32m    424\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n",
      "\u001B[31mAttributeError\u001B[39m: partially initialized module 'pandas' has no attribute 'core' (most likely due to a circular import)"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Data and processing functions\n",
    "\n",
    "def get_metric_interval(start_date: str,\n",
    "                        end_date: str,\n",
    "                        min_lead: int,\n",
    "                        max_lead: int,\n",
    "                        metric: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame containing `metric` between `start_date` and `end_date`\n",
    "    (inclusive) for all lead times between `min_lead` and `max_lead` (inclusive).\n",
    "    \"\"\"\n",
    "    metric = metric.capitalize()\n",
    "    if metric not in METRIC_FILES:\n",
    "        raise ValueError(f\"Unknown metric '{metric}'. Choose from {list(METRIC_FILES)}.\")\n",
    "\n",
    "    df = pd.read_csv(METRIC_FILES[metric])\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    start = pd.to_datetime(start_date)\n",
    "    end = pd.to_datetime(end_date)\n",
    "    df = df[(df['Date'] >= start) & (df['Date'] <= end)]\n",
    "    if df.empty:\n",
    "        raise KeyError(f\"No rows found between {start_date} and {end_date}.\")\n",
    "\n",
    "    lead_cols = [col for col in df.columns if col != 'Date' and min_lead <= int(col) <= max_lead]\n",
    "    if not lead_cols:\n",
    "        raise KeyError(f\"No lead‐time columns between {min_lead} and {max_lead}.\")\n",
    "\n",
    "    return df[['Date'] + sorted(lead_cols, key=int)]\n",
    "\n",
    "def get_metric_stats(start_date: str,\n",
    "                     end_date: str,\n",
    "                     min_lead: int,\n",
    "                     max_lead: int,\n",
    "                     metric: str) -> dict:\n",
    "    \"\"\"\n",
    "    Returns a dict with the overall min and max of `metric` in the specified\n",
    "    date and lead-time window, plus the Date/LeadTime where each occurs.\n",
    "    \"\"\"\n",
    "    df_int = get_metric_interval(start_date, end_date, min_lead, max_lead, metric)\n",
    "    df_long = df_int.melt(id_vars='Date', var_name='LeadTime', value_name='Value')\n",
    "    df_long['LeadTime'] = df_long['LeadTime'].astype(int)\n",
    "\n",
    "    idx_min = df_long['Value'].idxmin()\n",
    "    idx_max = df_long['Value'].idxmax()\n",
    "    min_row = df_long.loc[idx_min]\n",
    "    max_row = df_long.loc[idx_max]\n",
    "\n",
    "    return {\n",
    "        'min_value': min_row['Value'],\n",
    "        'min_date': min_row['Date'].strftime('%Y-%m-%d'),\n",
    "        'min_lead': int(min_row['LeadTime']),\n",
    "        'max_value': max_row['Value'],\n",
    "        'max_date': max_row['Date'].strftime('%Y-%m-%d'),\n",
    "        'max_lead': int(max_row['LeadTime'])\n",
    "    }\n",
    "\n",
    "def parse_args_with_defaults(args):\n",
    "    args.setdefault('min_lead', 0)\n",
    "    args.setdefault('max_lead', 150)\n",
    "    return args\n"
   ],
   "id": "33e2facc0ce437c0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let me clarify your request: You're asking about potential errors in the provided Jupyter notebook code, but there's no explicit error shown. Let me help by identifying potential issues and providing a check.\n",
    "\n"
   ],
   "id": "ed7b3e8088021f9c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The notebook has a few potential issues that should be checked:\n",
    "\n",
    "1. Model path verification is needed - the hardcoded path might not exist\n",
    "2. CSV files existence check is missing\n",
    "3. No error handling for file loading"
   ],
   "id": "c8f27e0299d7928"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Let's add a diagnostic cell to check these potential issues:\n",
    "\n"
   ],
   "id": "ba5d33a9b92e7a47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "\n",
    "# Check files existence\n",
    "files_status = {\n",
    "    path: os.path.exists(path) for path in METRIC_FILES.values()\n",
    "}\n",
    "\n",
    "# Check model path\n",
    "model_path = \"/Users/aryanharooni/models/Meta-Llama-3-8B-Instruct-v2.Q4_K_M.gguf\"\n",
    "model_exists = os.path.exists(model_path)\n",
    "\n",
    "print(\"Files status:\", files_status)\n",
    "print(\"Model exists:\", model_exists)"
   ],
   "id": "383b2e9a3f3fd2bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Let's also add a safer version of file loading with error handling:\n",
    "\n"
   ],
   "id": "feeac5779b1734cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def safe_load_metric_file(file_path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "        return pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error loading {file_path}: {str(e)}\")"
   ],
   "id": "2b0e085d153d6534"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test one of the files\n",
    "try:\n",
    "    df = safe_load_metric_file(METRIC_FILES[\"Rmsd\"])\n",
    "    print(\"Successfully loaded RMSD data\")\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "except Exception as e:\n",
    "    print(\"Error:\", str(e))\n"
   ],
   "id": "48e879b6c859363e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Llama model and prompts\n",
    "\n",
    "# Instantiate llama.cpp (edit path to your own model file if needed)\n",
    "llm = Llama(model_path=\"/Users/aryanharooni/models/Meta-Llama-3-8B-Instruct-v2.Q4_K_M.gguf\")\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a function-calling assistant. When the user asks a question about RMSD, Skil, or Bias metrics, reply ONLY with JSON in this exact format:\n",
    "{\"function\": \"get_metric_stats\", \"args\": {\"start_date\": \"<YYYY-MM-DD>\", \"end_date\": \"<YYYY-MM-DD>\", \"min_lead\": <int>, \"max_lead\": <int>, \"metric\": \"<metric>\"}}\n",
    "\n",
    "- Only use the function \"get_metric_stats\" to answer all queries.\n",
    "- If min_lead or max_lead is not specified, use min_lead=0 and max_lead=150.\n",
    "- Do NOT include explanations or any extra text, just the JSON.\n",
    "\"\"\"\n",
    "\n",
    "def call_llm(user_prompt):\n",
    "    prompt = f\"<|system|>\\n{system_prompt}\\n<|user|>\\n{user_prompt}\\n<|assistant|>\\n\"\n",
    "    output = llm(\n",
    "        prompt=prompt,\n",
    "        stop=[\"<|user|>\", \"<|system|>\"],\n",
    "        max_tokens=256,\n",
    "        temperature=0.2,\n",
    "    )[\"choices\"][0][\"text\"].strip()\n",
    "    try:\n",
    "        json_start = output.find('{')\n",
    "        json_data = output[json_start:]\n",
    "        return json.loads(json_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not parse LLM output: {output}\")\n",
    "        raise\n",
    "\n",
    "def generate_human_answer(user_prompt, stats, args):\n",
    "    answer_prompt = f\"\"\"\n",
    "You are an assistant that summarizes results for metrics queries.\n",
    "- Given a user question and a dictionary of statistics, answer concisely and clearly.\n",
    "- Only mention information relevant to the user's question.\n",
    "- If the user only asks for the max, do not mention the min, and vice versa.\n",
    "\n",
    "User question:\n",
    "{user_prompt}\n",
    "\n",
    "Statistics dictionary:\n",
    "{json.dumps(stats)}\n",
    "\n",
    "Date range: {args.get('start_date')} to {args.get('end_date')}\n",
    "Lead times: {args.get('min_lead')} to {args.get('max_lead')}\n",
    "Metric: {args.get('metric')}\n",
    "\"\"\"\n",
    "    output = llm(\n",
    "        prompt=f\"<|system|>\\n{answer_prompt}\\n<|assistant|>\\n\",\n",
    "        stop=[\"<|user|>\", \"<|system|>\"],\n",
    "        max_tokens=192,\n",
    "        temperature=0.2,\n",
    "    )[\"choices\"][0][\"text\"].strip()\n",
    "    return output\n"
   ],
   "id": "76797b3cace43fa7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run a query\n",
    "\n",
    "def ask_llm_metric_question(user_question):\n",
    "    try:\n",
    "        llm_response = call_llm(user_question)\n",
    "        fn = llm_response[\"function\"]\n",
    "        args = parse_args_with_defaults(llm_response[\"args\"])\n",
    "        if fn == \"get_metric_stats\":\n",
    "            stats = get_metric_stats(**args)\n",
    "            display(Markdown(f\"**Raw stats:** `{stats}`\"))\n",
    "            answer = generate_human_answer(user_question, stats, args)\n",
    "            display(Markdown(f\"**LLM Answer:** {answer}\"))\n",
    "        else:\n",
    "            display(Markdown(\"Unknown function.\"))\n",
    "    except Exception as ex:\n",
    "        display(Markdown(f\"**Error:** {ex}\"))\n",
    "\n",
    "# Example interactive call (uncomment to use)\n",
    "# ask_llm_metric_question(\"What is the maximum RMSD between June 2024 and July 2024?\")\n"
   ],
   "id": "d792cb4d3da22d59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Interactive input\n",
    "\n",
    "# To use interactively in the notebook\n",
    "user_question = input(\"Ask your metrics question: \")\n",
    "ask_llm_metric_question(user_question)\n"
   ],
   "id": "8f9eb07be7be19db"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
